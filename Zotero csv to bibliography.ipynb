{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"10ozgxoDkHcIKzDJO9LOGTFq5hR4zGLEB","timestamp":1730107340455}],"authorship_tag":"ABX9TyNm5jpZuapkKIF/MVh7NbMa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","!pip install groq\n","from groq import Groq\n","os.environ[\"GROQ_API_KEY\"] = \"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\"\n","client = Groq()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"whNIECD4aGqg","executionInfo":{"status":"ok","timestamp":1730108762473,"user_tz":-210,"elapsed":8166,"user":{"displayName":"Mahdi Abdollahzadeh","userId":"04069377538035711607"}},"outputId":"515884ee-1408-40c0-f470-251a48652cae"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from groq)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n","  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n","Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n","Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2\n"]}]},{"cell_type":"code","source":["df = pd.read_excel('EEG_Coma.xlsx')"],"metadata":{"id":"ofVG6RxojNUm","executionInfo":{"status":"ok","timestamp":1730108766649,"user_tz":-210,"elapsed":2692,"user":{"displayName":"Mahdi Abdollahzadeh","userId":"04069377538035711607"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Iterate through all the rows in the DataFrame\n","for row_index in range(1, 165):\n","    col_index_message = 10  # Assuming message is in the second column (index 1)\n","    col_index_response = 2  # Assuming you want to store the response in the third column (index 2)\n","\n","    # Access the message from the second column\n","    message = df.iloc[row_index, col_index_message]\n","\n","    # Generate the completion using your API call\n","    completion = client.chat.completions.create(\n","        model=\"llama3-70b-8192\",\n","        messages=[\n","            {\n","                \"role\": \"user\",\n","                \"content\": f\"This is an abstract of a scientific article. Extract the main objective of the article in a single short sentence: {message}\"\n","            }\n","        ],\n","        temperature=0.5,\n","        max_tokens=4000,\n","        top_p=1,\n","        stream=True,\n","        stop=None,\n","    )\n","    print(message)\n","    # Store the response\n","    response = \"\"\n","\n","    for chunk in completion:\n","        response += chunk.choices[0].delta.content or \"\"\n","\n","\n","    # Store the response in the third column (index 2) of the current row\n","    df.iloc[row_index, col_index_response] = response\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"XTC8FC-ozO7N","executionInfo":{"status":"error","timestamp":1730110209622,"user_tz":-210,"elapsed":7099,"user":{"displayName":"Mahdi Abdollahzadeh","userId":"04069377538035711607"}},"outputId":"94992eb7-d136-4a69-e0ab-6d8a2b87dcbb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["nan\n","nan\n","nan\n","nan\n","Abstract                            Background               To compare three computer-assisted quantitative electroencephalography (EEG) prediction models for the outcome prediction of comatose patients after cardiac arrest regarding predictive performance and robustness to artifacts.                                         Methods                                A total of 871 continuous EEGs recorded up to 3 days after cardiac arrest in intensive care units of five teaching hospitals in the Netherlands were retrospectively analyzed. Outcome at 6 months was dichotomized as “good” (Cerebral Performance Category 1–2) or “poor” (Cerebral Performance Category 3–5). Three prediction models were implemented: a logistic regression model using two quantitative features, a random forest model with nine features, and a deep learning model based on a convolutional neural network. Data from two centers were used for training and fivefold cross-validation (                 n                  = 663), and data from three other centers were used for external validation (                 n                  = 208). Model output was the probability of good outcome. Predictive performances were evaluated by using receiver operating characteristic analysis and the calculation of predictive values. Robustness to artifacts was evaluated by using an artifact rejection algorithm, manually added noise, and randomly flattened channels in the EEG.                                                        Results                                The deep learning network showed the best overall predictive performance. On the external test set, poor outcome could be predicted by the deep learning network at 24 h with a sensitivity of 54% (95% confidence interval [CI] 44–64%) at a false positive rate (FPR) of 0% (95% CI 0–2%), significantly higher than the logistic regression (sensitivity 33%, FPR 0%) and random forest models (sensitivity 13%, FPR, 0%) (                 p                  < 0.05). Good outcome at 12 h could be predicted by the deep learning network with a sensitivity of 78% (95% CI 52–100%) at a FPR of 12% (95% CI 0–24%) and by the logistic regression model with a sensitivity of 83% (95% CI 83–83%) at a FPR of 3% (95% CI 3–3%), both significantly higher than the random forest model (sensitivity 1%, FPR 0%) (                 p                  < 0.05). The results of the deep learning network were the least affected by the presence of artifacts, added white noise, and flat EEG channels.                                                        Conclusions               A deep learning model outperformed logistic regression and random forest models for reliable, robust, EEG-based outcome prediction of comatose patients after cardiac arrest.\n","In recent years, some electrophysiological analysis methods of consciousness have been proposed. Most of these studies are based on visual interpretation or statistical analysis, and there is hardly any work classifying the level of consciousness in a deep coma. In this study, we perform an analysis of electroencephalography complexity measures by quantifying features efficiency in differentiating patients in different consciousness levels. Several measures of complexity have been proposed to quantify the complexity of signals. Our aim is to lay the foundation of a system that will objectively define the level of consciousness by performing a complexity analysis of Electroencephalogram (EEG) signals. Therefore, a nonlinear analysis of EEG signals obtained with a recording scheme proposed by us from 39 patients with Glasgow Coma Scale (GCS) between 3 and 8 was performed. Various entropy values (approximate entropy, permutation entropy, etc.) obtained from different algorithms, Hjorth parameters, Lempel–Ziv complexity and Kolmogorov complexity values were extracted from the signals as features. The features were analyzed statistically and the success of features in classifying different levels of consciousness was measured by various classifiers. Consequently, levels of consciousness in deep coma (GCS between 3 and 8) were classified with an accuracy of 90.3%. To the authors’ best knowledge, this is the first demonstration of the discriminative nonlinear features extracted from tactile and auditory stimuli EEG signals in distinguishing different GCSs of comatose patients.\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-b761c878fcdc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Generate the completion using your API call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3-70b-8192\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         messages=[\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         )\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 936\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1025\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;31m# different thread if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         return self._request(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# Save the DataFrame to an Excel file\n","output_filename = 'updated_dataframe.xlsx'\n","df.to_excel(output_filename, index=False)\n","\n","print(f\"DataFrame saved to {output_filename}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7hbMbEO1FFYK","executionInfo":{"status":"ok","timestamp":1730109922184,"user_tz":-210,"elapsed":413,"user":{"displayName":"Mahdi Abdollahzadeh","userId":"04069377538035711607"}},"outputId":"8fe4281d-03f4-43fb-b738-0f4160652deb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame saved to updated_dataframe.xlsx\n"]}]}]}